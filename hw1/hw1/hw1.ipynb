{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "704f5db9",
   "metadata": {},
   "source": [
    "# 7. Hands-on with data\n",
    "\n",
    "In the following problem, you will use two simple datasets to walk through the steps of a standard machine learning workflow: inspecting your data, choosing a model, implementing it, and verifying its accuracy. \n",
    "We have provided two datasets in the form of numpy arrays: *dataset_1.npy* and *dataset_1.npy*. \n",
    "You can load each using NumPy's *np.load* method(https://numpy.org/doc/). \n",
    "You can plot figures using Matplotlib's *plt.plot* method(https://matplotlib.org/stable/users/explain/quick_start.html).\n",
    "\n",
    "Each dataset is a two-column array with the first column consisting of $n$ scalar inputs $X \\in \\R^{n \\times 1}$ and the second column consisting of $n$ scalar labels $Y \\in \\R^{n \\times 1}$. \n",
    "We denote each entry of $X$ and $Y$ with subscripts:\n",
    "$$\n",
    "X = \n",
    "    \\begin{bmatrix}\n",
    "        x_1 \\\\\n",
    "        x_2 \\\\\n",
    "        \\vdots \\\\\n",
    "        x_n \\\\\n",
    "    \\end{bmatrix}\n",
    "    ~~~~~~~~\n",
    "    Y = \n",
    "    \\begin{bmatrix}\n",
    "        y_1 \\\\\n",
    "        y_2 \\\\\n",
    "        \\vdots \\\\\n",
    "        y_n \\\\\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "and assume that $y_i$ is a (potentially stochastic) function of $x_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94ec7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621adcf3",
   "metadata": {},
   "source": [
    "(a) It is often useful to visually inspect your data and calculate simple statistics; this can detect dataset corruptions or inform your method.\n",
    "\n",
    "Notes: Your solution may make use of the NumPy library only for arithmetic operations, matrix-vector or matrix-matrix multiplications, matrix inversion, and elementwise exponentiation. It may not make use of library calls for calculating means, standard deviations, or the correlation coefficient itself directly.\n",
    "\n",
    " For both datasets:\n",
    "\n",
    "- (i) Plot the data as a scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a397968",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93220aee",
   "metadata": {},
   "source": [
    "- (ii) Calculate the correlation coefficient between X and Y:\n",
    "$$ \\rho_{X,Y} = \\frac{\\text{Cov}(X, Y)}{\\sigma_X \\sigma_Y} $$\n",
    "in which $\\text{Cov}(X, Y)$ is the covariance between $X$ and $Y$ and $\\sigma_X$ is the standard deviation of $X$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30609dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6642e6fa",
   "metadata": {},
   "source": [
    "(b) We would like to design a function that can predict $y_i$ given $x_i$ and then apply it to new inputs. This is a recurring theme in machine learning, and you will soon learn about a general-purpose framework for thinking about such problems. As a preview, we will now explore one of the simplest instantiations of this idea using the class of linear functions:\n",
    "$$\\hat{Y} = X w.$$\n",
    "The parameters of our function are denoted by $w \\in \\R$. It is common to denote predicted variants of quantities with a hat, so $\\hat{Y}$ is a predicted label whereas $Y$ is a ground truth label.\n",
    "We would like to find a $w^{*}$ that minimizes the **squared error** $\\mathcal{J}_\\text{SE}$ between predictions and labels:\n",
    "$$ w^* = \\argmin_{w} \\mathcal{J}_\\text{SE}(w) = \\argmin_w \\| Xw - Y \\|_2^2. $$\n",
    "Derive $\\nabla_w \\mathcal{J}_\\text{SE}(w)$ and set it equal to 0 to solve for $w^*$. (Note that this procedure for finding an optimum relies on the convexity of $\\mathcal{J}_\\text{SE}$. You do not need to show convexity here, but it is a useful exercise to convince yourself this is valid.)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c8031d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e0ba94",
   "metadata": {},
   "source": [
    "(c) Your solution $w^*$ should be a function of $X$ and $Y$. Implement it and report its **mean squared error**(MSE) for **dataset 1**. The mean squared error is the objective $\\mathcal{J}_\\text{SE}$ from part (b) divided by the number of datapoints:\n",
    "$$\\mathcal{J}_\\text{MSE}(w) = \\frac{1}{n} \\| Xw - Y \\|_2^2.$$\n",
    "Also visually inspect the model's quality by plotting a line plot of predicted $\\hat{y}$ for uniformly-spaced $x \\in [0, 10]$. Keep the scatter plot from part (a) in the background so that you can compare the raw data to your linear function. Does the function provide a good fit of the data? Why or why not?\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33cbbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8491a3a4",
   "metadata": {},
   "source": [
    "(d) We are now going to experiment with constructing new *features* for our model. That is, instead of considering models that are linear in the inputs, we will now consider models that are linear in some (potentially nonlinear) transformation of the data:\n",
    "$$\n",
    "\\hat{Y} = \\Phi w = \\begin{bmatrix}\n",
    "            \\phi(x_1)^\\top \\\\\n",
    "            \\phi(x_2)^\\top \\\\\n",
    "            \\vdots \\\\\n",
    "            \\phi(x_n)^\\top\n",
    "        \\end{bmatrix} w,\n",
    "$$\n",
    "where $\\phi(x_i), w \\in \\R^m$. Repeat part (c), providing both the mean squared error of your predictor and a plot of its predictions, for the following features on **dataset 1**:\n",
    "$$\n",
    "\\phi(x_i) = \\begin{bmatrix}\n",
    "            x_i \\\\\n",
    "            1\n",
    "        \\end{bmatrix}.\n",
    "$$\n",
    "How do the plotted function and mean squared error compare? (A single sentence will suffice.)\n",
    "\n",
    "*Hint:* the general form of your solution for $w^*$ is still valid, but you will now need to use features $\\Phi$ where you once used raw inputs $X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0cd463",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000dd5e3",
   "metadata": {},
   "source": [
    "(e) Now consider the quadratic features:\n",
    "    $$\n",
    "        \\phi(x_i) = \\begin{bmatrix}\n",
    "            x_i^2 \\\\\n",
    "            x_i \\\\\n",
    "            1\n",
    "        \\end{bmatrix}.\n",
    "    $$\n",
    "Repeat part (c) with these features on **dataset 1**, once again providing short commentary on any changes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c8a215",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2735ee2f",
   "metadata": {},
   "source": [
    "(f) Repeat parts (c) - (e) with **dataset 2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418f87aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7594f25b",
   "metadata": {},
   "source": [
    "(g) Finally, we would like to understand which features $\\Phi$ provide us with the best model. To that end, you will implement a method known as $k$-fold cross validation. The following are instructions for this method; deliverables for part (g) are at the end.\n",
    "\n",
    "- (i) Split **dataset 2** randomly into $k=4$ equal sized subsets. Group the dataset into 4 distinct training / validation splits by denoting each subset as the validation set and the remaining subsets as the training set for that split.\n",
    "\n",
    "- (ii) On each of the 4 training / validation splits, fit linear models using the following 5 polynomial feature sets:\n",
    "$$\n",
    "\\phi_1(x_i) = \\begin{bmatrix}\n",
    "                x_i \\\\\n",
    "                1\n",
    "            \\end{bmatrix}\n",
    "            ~~~~\n",
    "            \\phi_2(x_i) = \\begin{bmatrix}\n",
    "                x_i^2 \\\\\n",
    "                x_i \\\\\n",
    "                1\n",
    "            \\end{bmatrix}\n",
    "            ~~~~\n",
    "            \\phi_3(x_i) = \\begin{bmatrix}\n",
    "                x_i^3 \\\\\n",
    "                x_i^2 \\\\\n",
    "                x_i \\\\\n",
    "                1\n",
    "            \\end{bmatrix}\n",
    "            ~~~~\n",
    "            \\phi_4(x_i) = \\begin{bmatrix}\n",
    "                x_i^4 \\\\\n",
    "                x_i^3 \\\\\n",
    "                x_i^2 \\\\\n",
    "                x_i \\\\\n",
    "                1\n",
    "            \\end{bmatrix}\n",
    "            ~~~~\n",
    "            \\phi_5(x_i) = \\begin{bmatrix}\n",
    "                x_i^5 \\\\\n",
    "                x_i^4 \\\\\n",
    "                x_i^3 \\\\\n",
    "                x_i^2 \\\\\n",
    "                x_i \\\\\n",
    "                1\n",
    "            \\end{bmatrix} $$ \n",
    "This step will produce 20 distinct $w^*$ vectors: one for each dataset split and featurization $\\phi_j$. \n",
    "   \n",
    "- (iii) For each feature set $\\phi_j$, average the training and validation mean squared errors over all training splits.\n",
    "\n",
    "It is worth thinking about what this extra effort has bought us: by splitting the dataset into subsets, we were able to use all available datapoints for model fitting while still having held-out datapoints for evaluation for any particular model.\n",
    "\n",
    "**Deliverables for part (g):** Plot the training mean squared error and the validation mean squared error on the same plot as a function of the largest exponent in the feature set. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2863f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
